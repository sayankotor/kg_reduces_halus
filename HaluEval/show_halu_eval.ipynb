{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca04ce4-7f20-4643-9334-d8881f28a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('benchmark_ZsRE_ZsRE-test-all.json') as f:\n",
    "    a = json.load(f)\n",
    "    #line_content = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b90bc9e-d962-4d2e-9f7f-c7b9171f058e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3bafda0-0cf9-4f10-b3ae-872b9932dc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Nicolas Raffault',\n",
       " 'target_new': 'Arizona Coyotes',\n",
       " 'prompt': 'What team is Nicolas Raffault associated with?',\n",
       " 'ground_truth': ['Lyon OU'],\n",
       " 'rephrase_prompt': 'Which team is Nicolas Raffault associated with?',\n",
       " 'cond': 'Washington Capitals >> Arizona Coyotes || What team is Nicolas Raffault associated with?',\n",
       " 'locality': {'Relation_Specificity': [{'prompt': 'The given name of Nicolas Raffault is',\n",
       "    'ground_truth': ['Nicolas']},\n",
       "   {'prompt': 'Nicolas Raffault given name', 'ground_truth': ['Nicolas']}]},\n",
       " 'portability': {'Reasoning': [{'prompt': \"In which professional league does Nicolas Raffault's team compete?\",\n",
       "    'ground_truth': 'National Hockey League'}]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1dc942b-599d-46b4-963d-c62893906c49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/chekalina_kbqa/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae63e3fe-d181-4f98-856f-008102b4f6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Suggan Buggan River',\n",
       " 'target_new': 'Bass Strait',\n",
       " 'prompt': 'What body of water does Suggan Buggan River join?',\n",
       " 'ground_truth': ['Snowy River'],\n",
       " 'rephrase_prompt': 'Where does the Suggan Buggan River flow into?',\n",
       " 'cond': 'Allegheny River >> Bass Strait || What body of water does Suggan Buggan River join?',\n",
       " 'locality': {'Relation_Specificity': [{'prompt': 'The drainage basin of Suggan Buggan River is',\n",
       "    'ground_truth': ['Murray–Darling basin']},\n",
       "   {'prompt': 'Suggan Buggan River drainage basin',\n",
       "    'ground_truth': ['Murray–Darling basin']}]},\n",
       " 'portability': {'Subject_Aliasing': [{'prompt': 'What is Suggan Buggan?',\n",
       "    'ground_truth': 'Bass Strait'}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029ecb97-5290-4d93-b9c7-72c697b169e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a33191-e257-4284-8207-1fd4b5b3f851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ea6484-b015-44da-bcec-935e840d3337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement https-proxy-agent (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for https-proxy-agent\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install https-proxy-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af37c339-0540-47b3-b4c1-a15524041041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (792525794.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_42514/792525794.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import http from 'http';\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import http from http\n",
    "import HttpsProxyAgent from https-proxy-agent;\n",
    "\n",
    "// Configure the default for all requests:\n",
    "const openai = new OpenAI({\n",
    "  httpAgent: new HttpsProxyAgent('http://168.63.76.32:3128'),\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58886f58-ac5a-4c4d-a7ec-856113246bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Who should I be, as I answer your prompt? asshole\n",
      "What prompt do you want me to do? to dance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are a asshole', 'to dance')\n"
     ]
    }
   ],
   "source": [
    "gpt_assistant_prompt = \"You are a \" + input (\"Who should I be, as I answer your prompt?\") \n",
    "gpt_user_prompt = input (\"What prompt do you want me to do?\") \n",
    "gpt_prompt = gpt_assistant_prompt, gpt_user_prompt\n",
    "print(gpt_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6afe5bf-58f0-4c0a-8914-5b05baa939ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31566/2769554960.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.mlspace/envs/chekalina_kbqa/lib/python3.7/site-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.mlspace/envs/chekalina_kbqa/lib/python3.7/site-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mChatCompletion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionChunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         )\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.mlspace/envs/chekalina_kbqa/lib/python3.7/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     def patch(\n",
      "\u001b[0;32m~/.mlspace/envs/chekalina_kbqa/lib/python3.7/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m             \u001b[0mremaining_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremaining_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.mlspace/envs/chekalina_kbqa/lib/python3.7/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}"
     ]
    }
   ],
   "source": [
    "message=[{\"role\": \"assistant\", \"content\": gpt_assistant_prompt}, {\"role\": \"user\", \"content\": gpt_user_prompt}]\n",
    "temperature=0.2\n",
    "max_tokens=256\n",
    "frequency_penalty=0.0\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages = message,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    frequency_penalty=frequency_penalty\n",
    ")\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd98aa-2176-497d-92ad-f9c1606b04a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-chekalina_kbqa]",
   "language": "python",
   "name": "conda-env-.mlspace-chekalina_kbqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

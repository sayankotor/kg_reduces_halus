{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f5a1e2-3069-4e4f-b972-ca67e249d6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-12 00:53:56,089] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/../lib/gcc/x86_64-conda-linux-gnu/12.4.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/../lib/gcc/x86_64-conda-linux-gnu/12.4.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17368593e466436190458b707973ff54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in tokenizer: 32000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from urllib.request import urlopen\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading some sources of the projection adapter and image encoder\n",
    "#hf_hub_download(repo_id=\"AIRI-Institute/OmniFusion\", filename=\"models.py\", local_dir='./')\n",
    "#from models import CLIPVisionTower\n",
    "\n",
    "DEVICE = \"cuda:1\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "model_id = \"unsloth/llama-2-7b\"  # or \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": DEVICE}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "unk_id = tokenizer.encode(\"<unk>\", add_special_tokens=False)[0]\n",
    "tokenizer.pad_token_id = 2\n",
    "tokenizer.eos_token_id = 0\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "N_EMBEDDINGS = model.model.embed_tokens.weight.shape[0]\n",
    "print(\"Number of embeddings in tokenizer:\", N_EMBEDDINGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb6e1e7-500b-4928-8e9e-a638b30f2019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7ff1d19b6d47cf974e054a1a8ee190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f976e5d8ec42b4af4db2f9a2656d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9679d0604247678e2233fb96ef70e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Sayankotor/small_wikipaper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac3706c-c3b3-42bb-a61d-7f74b9bcf1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        # Ensure 'entities' is parsed if stored as string\n",
    "        self.ds = [\n",
    "            item for item in ds['train']\n",
    "            if len(literal_eval(item['entities'])) > 1\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        try:\n",
    "            text = item['text'][:2048]\n",
    "        except Exception:\n",
    "            print(\"Bad example (no text):\", item)\n",
    "            text = \"Bad example\"\n",
    "        \n",
    "        ents = literal_eval(item['entities'])\n",
    "        embs = np.array(literal_eval(item['entity_embs']))[:200]\n",
    "\n",
    "        return text, ents, embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ba91ac-9ff5-4e45-a2a9-042c5a8dde08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "subset_indices = list(range(16000))\n",
    "random.shuffle(subset_indices)\n",
    "dataset = PretrainDataset(ds)\n",
    "# Wrap the dataset with Subset\n",
    "subset = Subset(dataset, subset_indices)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(subset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e06bac0-8ec3-443f-944f-cf7b2ca84768",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Actresses (Catalan: Actrius) is a 1997 Catalan language Spanish drama film produced and directed by Ventura Pons and based on the award-winning stage play E.R. by Josep Maria Benet i Jornet. The film has no male actors, with all roles played by females. The film was produced in 1996. == Synopsis == In order to prepare herself to play a role commemorating the life of legendary actress Empar Ribera, young actress (Mercè Pons) interviews three established actresses who had been the Ribera\\'s pupils: the international diva Glòria Marc (Núria Espert), the television star Assumpta Roca (Rosa Maria Sardà), and dubbing director Maria Caminal (Anna Lizaran). == Cast == * Núria Espert as Glòria Marc * Rosa Maria Sardà as Assumpta Roca * Anna Lizaran as Maria Caminal * Mercè Pons as Estudiant == Recognition == === Screenings === Actrius screened in 2001 at the Grauman\\'s Egyptian Theatre in an American Cinematheque retrospective of the works of its director. The film had first screened at the same location in 1998. It was also shown at the 1997 Stockholm International Film Festival. === Reception === In Movie - Film - Review, Christopher Tookey wrote that though the actresses were \"competent in roles that may have some reference to their own careers\", the film \"is visually unimaginative, never escapes its stage origins, and is almost totally lacking in revelation or surprising incident\". Noting that there were \"occasional, refreshing moments of intergenerational bitchiness\", they did not \"justify comparisons to All About Eve\", and were \"insufficiently different to deserve critical parallels with Rashomon\". He also wrote that The Guardian called the film a \"slow, stuffy chamber-piece\", and that The Evening Standard stated the film\\'s \"best moments exhibit the bitchy tantrums seething beneath the threesome\\'s composed veneers\". MRQE wrote \"This cinematic adaptation of a theatrical work is true to the original, but does not stray far from a theatrical rendering of the story.\" === Awards and nominations === * 1997, won \\'Best Catala',\n",
       " ['Catalan language',\n",
       "  'Ventura Pons',\n",
       "  'Josep Maria Benet i Jornet',\n",
       "  'Núria Espert',\n",
       "  'Rosa Maria Sardà',\n",
       "  'Anna Lizaran',\n",
       "  'Mercè Pons',\n",
       "  'Stockholm International Film Festival',\n",
       "  \"Grauman's Egyptian Theatre\",\n",
       "  'American Cinematheque',\n",
       "  'All About Eve',\n",
       "  'Rashomon',\n",
       "  'The Guardian',\n",
       "  'The Evening Standard',\n",
       "  'MRQE',\n",
       "  'Butaca Awards',\n",
       "  'Goya Awards'],\n",
       " array([[ 0.1537,  0.5156,  0.4152, ...,  0.7399,  0.1703, -0.0951],\n",
       "        [-0.2455, -0.373 ,  0.4601, ...,  0.1081,  0.1381,  0.0978],\n",
       "        [-0.5763,  0.2887,  0.5739, ...,  0.0209,  0.01  ,  0.0645],\n",
       "        ...,\n",
       "        [-0.3942,  0.136 ,  0.3044, ...,  0.2207,  0.0908, -0.1589],\n",
       "        [-0.1908, -0.2533,  0.1643, ...,  0.2528,  0.4068,  0.3871],\n",
       "        [ 0.2057, -0.4914,  0.4807, ...,  0.3731,  0.1044, -0.0531]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaad73a-8d76-43ab-9b13-b5dd647e5243",
   "metadata": {},
   "source": [
    "### Train adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551fbeb-6ebb-4081-81bf-7a523c44398d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import (Adafactor, AdafactorSchedule,\n",
    "                                       get_cosine_schedule_with_warmup)\n",
    "\n",
    "import gc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "kg_emb_dim = 200\n",
    "llama_emb_dim = 4096\n",
    "\n",
    "\n",
    "kg_start_emb = torch.normal(\n",
    "    torch.zeros(llama_emb_dim), \n",
    "    torch.ones(llama_emb_dim) / llama_emb_dim**0.5\n",
    ").to(device=DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "kg_end_emb = torch.normal(\n",
    "    torch.zeros(llama_emb_dim), \n",
    "    torch.ones(llama_emb_dim) / llama_emb_dim**0.5\n",
    ").to(device=DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "projection = nn.Linear(kg_emb_dim, llama_emb_dim).to(device=DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "kg_start_emb.requires_grad_()\n",
    "kg_end_emb.requires_grad_()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "    \n",
    "lr = 5e-3\n",
    "weight_decay = 1e-3\n",
    "trainable_parameters = [kg_start_emb] + [kg_end_emb] + list(projection.parameters())\n",
    "\n",
    "opt = AdamW(trainable_parameters, lr=lr, weight_decay=weight_decay)\n",
    "loss_fct = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=unk_id)\n",
    "\n",
    "grad_accum = 256\n",
    "\n",
    "loss_best = 1000.0\n",
    "\n",
    "losses = []\n",
    "losses_batch = []\n",
    "iters = 0\n",
    "n_iters = len(dataloader)\n",
    "scheduler = get_cosine_schedule_with_warmup(opt, num_warmup_steps=n_iters // grad_accum * 0.01, num_training_steps=n_iters // grad_accum)\n",
    "\n",
    "for epoch in range(1):\n",
    "    i = 0 \n",
    "    for step in tqdm.notebook.tqdm(range(n_iters)):\n",
    "        batch = next(iter(dataloader))\n",
    "        text, ents, embs = batch\n",
    "        \n",
    "        model.eval()\n",
    "        model.requires_grad = False\n",
    "        opt.zero_grad()\n",
    "    \n",
    "        prompt = f\"Continue this text:\\n\\n{text[0]}\"\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")[..., :2048].to(device=DEVICE)\n",
    "\n",
    "            text_ids = tokenizer.encode(prompt, add_special_tokens=False)[:2048]\n",
    "            \n",
    "            # Step 2: Add special tokens manually\n",
    "            text_ids += [tokenizer.eos_token_id]\n",
    "            \n",
    "            # Step 3: Convert to tensor\n",
    "            text_ids = torch.tensor([text_ids], device=DEVICE)\n",
    "            \n",
    "            half = text_ids.shape[1] // 2  # use shape[1] for token length\n",
    "            input_embeddings = model.model.embed_tokens(text_ids[:, :half])\n",
    "            output_embeddings = model.model.embed_tokens(text_ids[:, half:])\n",
    "            \n",
    "        try:\n",
    "            m = batch[2].mean(2, keepdim=True)\n",
    "            s = batch[2].std(2, unbiased=False, keepdim=True)\n",
    "            batch[2] = (batch[2] - m) / (s + 1e-6)\n",
    "        except:\n",
    "            print (\"except\", batch[2].shape)\n",
    "        try:\n",
    "            projected_kg_embeddings = projection(batch[2].to(\n",
    "                        device=DEVICE, dtype=model.dtype\n",
    "                    ))\n",
    "        except Exception as e:\n",
    "            print(\"❌ Error projecting KG embeddings\")\n",
    "            print(\"embs.shape:\", batch[2].shape)\n",
    "            print(\"Exception:\", str(e))\n",
    "            continue\n",
    "        \n",
    "        embeddings1 = torch.cat(\n",
    "                [\n",
    "                    input_embeddings,\n",
    "                    kg_start_emb[None, None, ...],\n",
    "                    projected_kg_embeddings,\n",
    "                    kg_end_emb[None, None, ...],\n",
    "                    output_embeddings\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        \n",
    "        #mask = torch.full(embeddings1.shape, False)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=model.dtype):\n",
    "            logits = model(inputs_embeds=torch.cat(\n",
    "                [\n",
    "                    input_embeddings,\n",
    "                    kg_start_emb[None, None, ...],\n",
    "                    projected_kg_embeddings,\n",
    "                    kg_end_emb[None, None, ...],\n",
    "                    output_embeddings\n",
    "                ],\n",
    "                dim=1,\n",
    "            ), output_hidden_states=True).get(\"logits\")\n",
    "            # loss only for answer part & backward\n",
    "\n",
    "            logits = logits[..., -output_embeddings.shape[1]:-1, :].contiguous()\n",
    "            labels = text_ids[:, half+1:].contiguous()\n",
    "\n",
    "            if torch.isnan(logits).any():\n",
    "                print(\"⚠️ NaN in logits\")\n",
    "            \n",
    "\n",
    "            loss = loss_fct(logits.permute(0, 2, 1), labels).mean()\n",
    "\n",
    "        if model.dtype == torch.float16:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        losses_batch.append(loss.item())\n",
    "        \n",
    "        if (step% 1000 == 0):         \n",
    "            out = model.generate(inputs_embeds=embeddings1, max_new_tokens = half)\n",
    "            generated_texts2 = tokenizer.batch_decode(out)[0]\n",
    "            print (\"\\n first part \\n\")\n",
    "            print(tokenizer.decode(text_ids[0, :half].tolist(), skip_special_tokens=True))\n",
    "            print(\"\\n last part \\n\")\n",
    "            print(tokenizer.decode(text_ids[0, half:].tolist(), skip_special_tokens=True))\n",
    "            print (\"\\n continue\", generated_texts2)\n",
    "            print (\"\\n\")\n",
    "            \n",
    "            print (\"loss\", np.mean(losses_batch))\n",
    "            print ('lr', scheduler.get_lr()[0], step, flush = True)\n",
    "            plt.title(\"train loss\\n\" + f\"\\n\\nEpoch [{epoch}], iter [{iters}/{n_iters}]\")\n",
    "            accum_loss = np.mean(losses_batch)\n",
    "            losses.append(accum_loss)\n",
    "            plt.semilogy(losses)\n",
    "            plt.grid()\n",
    "            plt.savefig(f\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/loss1_llama2.png\")\n",
    "            plt.close(\"all\")\n",
    "\n",
    "\n",
    "        if iters % grad_accum == 0 and iters > 0:\n",
    "            if model.dtype == torch.float16:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                opt.step()\n",
    "            opt.zero_grad()\n",
    "            scheduler.step()\n",
    "            accum_loss = np.mean(losses_batch)\n",
    "            losses.append(accum_loss)\n",
    "            losses_batch = []\n",
    "\n",
    "            if accum_loss < loss_best:\n",
    "                loss_best = accum_loss\n",
    "                torch.save(projection, f\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/projection_llama2\")\n",
    "                torch.save(kg_start_emb, f\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/SOI_llama2.pt\")\n",
    "                torch.save(kg_end_emb, f\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/EOI_llama2.pt\")\n",
    "            \n",
    "            \n",
    "            #gc.collect()\n",
    "        \n",
    "        iters += 1\n",
    "\n",
    "        # model inference to get\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af458aa-20e7-4d8b-b88e-52a0561fd398",
   "metadata": {},
   "source": [
    "## QA sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "139f3ed3-2cdb-4bc1-9733-f0afd777a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.read_csv('/home/jovyan/shares/SR004.nfs2/chekalina/check_halu/83000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1de77c14-202b-4e5f-bbaf-79ae9aac4614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wapack Wilderness and Lake Monomonac share what state?']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "tds = Dataset.from_pandas(pd_df)\n",
    "\n",
    "tds['question'][99:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "152dd3ac-55fa-4ef8-9857-cb9c95fcb602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_answer(ans):\n",
    "    try:\n",
    "        # Remove punctuation, convert to lowercase\n",
    "        cleaned = ans.strip().lower().replace(\"-\", \" \").replace(\",\", \"\")\n",
    "        # Try to parse as number\n",
    "        num = w2n.word_to_num(cleaned)\n",
    "        return str(num)\n",
    "    except:\n",
    "        return ans  # leave unchanged if not a number\n",
    "\n",
    "# Apply to dataset\n",
    "for ind, elem in enumerate(tds[\"question\"]):\n",
    "    if \"how many\" in elem.lower() or \"how much\" in elem.lower():\n",
    "        old_ans = tds[\"answer\"][ind]\n",
    "        new_ans = normalize_answer(old_ans)\n",
    "        print(f\"Q: {elem}\\nOriginal: {old_ans} → Normalized: {new_ans}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26f05892-5444-4536-b4f4-4b59af7b8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import (Adafactor, AdafactorSchedule,\n",
    "                                       get_cosine_schedule_with_warmup)\n",
    "\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        self.ds = [\n",
    "            item for item in ds if len(literal_eval(item['ents'])) > 1\n",
    "        ]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ents = literal_eval(self.ds[idx]['ents'])\n",
    "        try:\n",
    "            lst = literal_eval(self.ds[idx]['embs'])\n",
    "            lst = [elem for elem in lst if elem != -111]\n",
    "            embs = np.array(lst)\n",
    "            #embs = np.reshape(embs, (-1, 200)) \n",
    "        except:\n",
    "            print (self.ds[idx]['embs'])\n",
    "        \n",
    "        return self.ds[idx]['question'], self.ds[idx]['answer'], ents, embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ccc67d1b-bc59-47be-84fe-7bee29da790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "subset_indices = list(range(8000))\n",
    "random.shuffle(subset_indices)\n",
    "dataset = PretrainDataset(tds)\n",
    "# Wrap the dataset with Subset\n",
    "subset = Subset(dataset, subset_indices)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(subset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e985dfb5-8520-4e39-8357-613bf4ee8f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ruth, Caldwell County, Kentucky is located near a national forest established in which year ?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7dd5a4fe-9819-4b94-ad4d-840cefdcc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tds[2]['answer']\n",
    "input_ids = tokenizer(text, add_special_tokens=False)['input_ids']\n",
    "input_ids = input_ids + [tokenizer.eos_token_id]\n",
    "input_tensor = torch.tensor([input_ids]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c67c6e4e-83e6-41dc-9093-94f76d10fcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3082,    0]], device='cuda:1')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec10afa-7f33-4601-8b7d-3ef005ed27ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6348b2af0add45ebb82b1ab60e74e639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ents ['Libocedrus', 'Helianthus']\n",
      "You are a knowledgeable assistant. Answer the question with a short, simple response. Avoid explanations.\n",
      "\n",
      "Are Libocedrus and Helianthus both types of plants?\n",
      "\n",
      "Answer:\n",
      "\n",
      " last part \n",
      "\n",
      "yes<unk>\n",
      "\n",
      " continue \n",
      "\n",
      "Comment: Are Libocedrus and Helianthus both types of plants?\n",
      "\n",
      "Answer: Are Libocedrus and Helianthus both types of plants?\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Comment: Are Lib\n",
      "\n",
      "\n",
      "loss 2.984375\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "ents ['Lyman James Briggs', 'Great Depression']\n",
      "You are a knowledgeable assistant. Answer the question with a short, simple response. Avoid explanations.\n",
      "\n",
      "What years would Lyman James Briggs have been the director of the National Bureau of Standards, if he was a  a director of the National Bureau of Standards during the Great Depression?\n",
      "\n",
      "Answer:\n",
      "\n",
      " last part \n",
      "\n",
      "mostly during the 1930s<unk>\n",
      "\n",
      " continue \n",
      "\n",
      "Comment: Hi! Welcome to the site! It looks like you're answering a question that's 10 years old, but you can still edit it to improve it. Please do!\n",
      "\n",
      "Comment: I'm not sure that this is an answer to the question.  It's a good answer to a different question.  I think that the question is \"what years\n",
      "\n",
      "\n",
      "loss 2.120780276325014\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "ents ['A Web of Air', 'Mortal Engines Quartet', 'Mortal Engines Quartet']\n",
      "You are a knowledgeable assistant. Answer the question with a short, simple response. Avoid explanations.\n",
      "\n",
      "What is the first book in the prequel series to the four books referred to also as the Predator Cities Quartet by Philip Reeve?\n",
      "\n",
      "Answer:\n",
      "\n",
      " last part \n",
      "\n",
      "Fever Crumb<unk>\n",
      "\n",
      " continue \n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      "\\end{blockquote}\n",
      "\n",
      "Comment: That's it, thanks.\n",
      "\n",
      "Answer: The answer is\n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      " \\em{Mortal Engines}\n",
      "\\end{blockquote}\n",
      "\n",
      "I found this by using the following method:\n",
      "\\\n",
      "\n",
      "\n",
      "loss 1.40340758797538\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "ents ['I Wanna Know You', 'David Archuleta']\n",
      "You are a knowledgeable assistant. Answer the question with a short, simple response. Avoid explanations.\n",
      "\n",
      "In what year was the singer-songwriter who did the duet \"I Wanna Know You\" with Hannah Montana born?\n",
      "\n",
      "Answer:\n",
      "\n",
      " last part \n",
      "\n",
      "1990<unk>\n",
      "\n",
      " continue \n",
      "\n",
      "Comment: That was correct.  Thank you.  I have a feeling I'm going to be in the minority here but I really like this game.  I like the fact that it's not just trivia but it's a little bit of everything.  I also like that the questions\n",
      "\n",
      "\n",
      "loss 1.1203206200035487\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "ents ['The Three Caballeros', 'Aliens of the Deep']\n",
      "You are a knowledgeable assistant. Answer the question with a short, simple response. Avoid explanations.\n",
      "\n",
      "Which film was released first, The Three Caballeros or Aliens of the Deep?\n",
      "\n",
      "Answer:\n",
      "\n",
      " last part \n",
      "\n",
      "The Three Caballeros<unk>\n",
      "\n",
      " continue \n",
      "\n",
      "### 15\n",
      "\n",
      "Which film was released first, The Three Caballeros or Aliens of the Deep?\n",
      "\n",
      "Answer: The Three Caballeros\n",
      "\n",
      "### 16\n",
      "\n",
      "Which film was released first, The Three Caball\n",
      "\n",
      "\n",
      "loss 1.14192261453007\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "ents ['Kim Young-kwang (actor)', 'Hot Young Bloods', 'Hot Young Bloods']\n",
      "You are a knowledgeable assistant. Answer the question with a short, simple response. Avoid explanations.\n",
      "\n",
      "Kim Young-kwang starred in which 2014 teen romantic comedy film?\n",
      "\n",
      "Answer:\n",
      "\n",
      " last part \n",
      "\n",
      "Hot Young Bloods<unk>\n",
      "\n",
      " continue \n",
      "\n",
      "### Question 4\n",
      "\n",
      "You are a knowledgeable assistant. Answer the question with a short, simple response. Avoid explanations.\n",
      "\n",
      "Kim Young-kwang starred in which 2014 teen romantic comedy film?\n",
      "\n",
      "Answer: Hot\n",
      "\n",
      "\n",
      "loss 1.1066928282748945\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
      "ents ['Tukwila station', 'Westfield Southcenter']\n",
      "You are a knowledgeable assistant. Answer the question with a short, simple response. Avoid explanations.\n",
      "\n",
      "Who owns The mall that is a 5 minute drive from Tukwila station?\n",
      "\n",
      "Answer:\n",
      "\n",
      " last part \n",
      "\n",
      "Westfield Group<unk>\n",
      "\n",
      " continue \n",
      "\n",
      "Comment: You are correct. Thank you!\n",
      "\n",
      "Comment: Welcome!</s>\n",
      "\n",
      "\n",
      "loss 1.0673981942236423\n",
      "❌ Projection failed: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n"
     ]
    }
   ],
   "source": [
    "#projection = torch.load(\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/projection_llama2\", map_location=DEVICE)\n",
    "#kg_start_emb = torch.load(\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/SOI_llama2.pt\", map_location=DEVICE)\n",
    "#kg_end_emb = torch.load(\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/EOI_llama2.pt\", map_location=DEVICE)\n",
    "\n",
    "kg_emb_dim = 200\n",
    "llama_emb_dim = 4096\n",
    "\n",
    "kg_start_emb = torch.normal(\n",
    "    torch.zeros(llama_emb_dim), \n",
    "    torch.ones(llama_emb_dim) / llama_emb_dim**0.5\n",
    ").to(device=DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "kg_end_emb = torch.normal(\n",
    "    torch.zeros(llama_emb_dim), \n",
    "    torch.ones(llama_emb_dim) / llama_emb_dim**0.5\n",
    ").to(device=DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "projection = nn.Linear(kg_emb_dim, llama_emb_dim).to(device=DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "kg_start_emb.requires_grad_()\n",
    "kg_end_emb.requires_grad_()\n",
    "projection.requires_grad_()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "lr = 5e-3\n",
    "weight_decay = 1e-3\n",
    "trainable_parameters = [kg_start_emb] + [kg_end_emb] + list(projection.parameters())\n",
    "\n",
    "opt = AdamW(trainable_parameters, lr=lr, weight_decay=weight_decay)\n",
    "loss_fct = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=unk_id)\n",
    "\n",
    "grad_accum = 256\n",
    "\n",
    "loss_best = 1000.0\n",
    "\n",
    "losses = []\n",
    "losses_batch = []\n",
    "iters = 0\n",
    "n_iters = len(dataloader)\n",
    "scheduler = get_cosine_schedule_with_warmup(opt, num_warmup_steps=n_iters // grad_accum * 0.01, num_training_steps=n_iters // grad_accum)\n",
    "\n",
    "for epoch in range(1):\n",
    "    i = 0 \n",
    "    for step in tqdm.notebook.tqdm(range(n_iters)):\n",
    "       \n",
    "        \n",
    "        batch = next(iter(dataloader))\n",
    "        question, answer, ents, embs = batch\n",
    "        ents = [ent[0] for ent in ents]\n",
    "        \n",
    "        model.eval()\n",
    "        model.requires_grad = False\n",
    "        #opt.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            #print (question, answer)\n",
    "            prompt = f\"You are a knowledgeable assistant. Answer the question with a short, simple response. Avoid explanations.\\n\\n{question[0]}\\n\\nAnswer:\"\n",
    "\n",
    "            input_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "            output_ids = tokenizer.encode(answer[0], add_special_tokens=False)\n",
    "            \n",
    "            # Step 2: Add special tokens manually\n",
    "            output_ids += [tokenizer.eos_token_id]\n",
    "            \n",
    "            # Step 3: Convert to tensor\n",
    "            text_ids_in = torch.tensor([input_ids], device=DEVICE)\n",
    "            text_ids_out = torch.tensor([output_ids], device=DEVICE)\n",
    "\n",
    "            \n",
    "            input_embeddings = model.model.embed_tokens(text_ids_in)\n",
    "            output_embeddings = model.model.embed_tokens(text_ids_out)\n",
    "            \n",
    "        if (len(text_ids_out[0]) <= 1):\n",
    "            continue    \n",
    "        try:\n",
    "            # Normalize input embeddings (layer-wise across hidden dim)\n",
    "            embs = embs.to(device=DEVICE, dtype=model.dtype)\n",
    "            m = embs.mean(2, keepdim=True)\n",
    "            s = embs.std(2, unbiased=False, keepdim=True)\n",
    "            embs = (embs - m) / (s + 1e-6)\n",
    "        \n",
    "            # Map to LLM space\n",
    "            projected_kg_embeddings = projection(embs)\n",
    "        \n",
    "            # Optionally re-normalize output to match LLaMA token embedding scale\n",
    "            # This can help if your LLM embeddings have a stable target variance\n",
    "            #target_std = 1.0\n",
    "            #target_mean = 0.0\n",
    "            #proj_mean = projected_kg_embeddings.mean(dim=-1, keepdim=True)\n",
    "            #proj_std = projected_kg_embeddings.std(dim=-1, keepdim=True, unbiased=False)\n",
    "            #projected_kg_embeddings = ((projected_kg_embeddings - proj_mean) / (proj_std + 1e-6)) * target_std + target_mean\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"❌ Projection failed:\", e)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #mask = torch.full(embeddings1.shape, False)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=model.dtype):\n",
    "            logits = model(inputs_embeds=torch.cat(\n",
    "                [\n",
    "                    input_embeddings,\n",
    "                    kg_start_emb[None, None, ...],\n",
    "                    projected_kg_embeddings,\n",
    "                    kg_end_emb[None, None, ...],\n",
    "                    output_embeddings\n",
    "                ],\n",
    "                dim=1,\n",
    "            ), output_hidden_states=True).get(\"logits\")\n",
    "            \n",
    "            num_output_tokens = output_embeddings.shape[1]\n",
    "            #print (\"logits.shape\", logits.shape)\n",
    "            logits = logits[..., -1-output_embeddings.shape[1]:-1, :].contiguous()\n",
    "            labels = text_ids_out.contiguous()\n",
    "            #print (\"logits.shape\", logits.shape)\n",
    "            #print (\"labels.shape\", labels.shape)\n",
    "            \n",
    "            loss = loss_fct(logits.permute(0, 2, 1), labels).mean()\n",
    "            losses_batch.append(loss.item())\n",
    "            \n",
    "            if (labels >= logits.shape[-1]).any():\n",
    "                print(\"❌ Labels contain values outside valid range!\")\n",
    "            if torch.isnan(logits).any():\n",
    "                print(\"⚠️ NaN in logits\")\n",
    "            if torch.isnan(text_ids_out).any():\n",
    "                print(\"⚠️ NaN in labels\")\n",
    "\n",
    "            \n",
    "        if (step% 1000 == 0): \n",
    "            embeddings1 = torch.cat(\n",
    "                [\n",
    "                    input_embeddings,\n",
    "                    kg_start_emb[None, None, ...],\n",
    "                    projected_kg_embeddings,\n",
    "                    kg_end_emb[None, None, ...],\n",
    "                    output_embeddings\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            print (\"ents\", ents)\n",
    "            out = model.generate(inputs_embeds=embeddings1, max_new_tokens = embeddings1.shape[1])\n",
    "            generated_texts = tokenizer.batch_decode(out)[0]\n",
    "            print (tokenizer.batch_decode(text_ids_in)[0])\n",
    "            print (\"\\n last part \\n\")\n",
    "            print (tokenizer.batch_decode(text_ids_out)[0])\n",
    "            print (\"\\n continue\", generated_texts)\n",
    "            print (\"\\n\")\n",
    "            \n",
    "            print (\"loss\", np.mean(losses_batch))\n",
    "            plt.title(\"train loss\\n\" + f\"\\n\\nEpoch [{epoch}], iter [{iters}/{n_iters}]\")\n",
    "            accum_loss = np.mean(losses_batch)\n",
    "            losses.append(accum_loss)\n",
    "            plt.semilogy(losses)\n",
    "            plt.grid()\n",
    "            plt.savefig(f\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/loss3.png\")\n",
    "            plt.close(\"all\")\n",
    "\n",
    "        if model.dtype == torch.float16:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "\n",
    "\n",
    "        if iters % grad_accum == 0 and iters > 0:\n",
    "            if model.dtype == torch.float16:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                opt.step()\n",
    "            opt.zero_grad()\n",
    "            scheduler.step()\n",
    "            accum_loss = np.mean(losses_batch)\n",
    "            losses.append(accum_loss)\n",
    "            losses_batch = []\n",
    "\n",
    "            if accum_loss < loss_best:\n",
    "                loss_best = accum_loss\n",
    "                torch.save(projection, f\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/projection_llama2_qa\")\n",
    "                torch.save(kg_start_emb, f\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/SOI_llama2_qa.pt\")\n",
    "                torch.save(kg_end_emb, f\"/home/jovyan/shares/SR004.nfs2/chekalina/kg_reduces_halus/notebook_new/ckpts/EOI_llama2_qa.pt\")\n",
    "            \n",
    "            \n",
    "            #gc.collect()\n",
    "        \n",
    "        iters += 1\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3b037-7ad0-4bb4-86f1-4ff0551be026",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ids = tokenizer(\"The capital of France is\", return_tensors=\"pt\").input_ids.to(device)\n",
    "embs = model.model.embed_tokens(tok_ids)\n",
    "print(embs.mean().item(), embs.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff3053e-82ef-4fd3-a276-bd0520dea488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import collections\n",
    "import re\n",
    "from typing import Union, List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_and_tokenize_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text.strip().lower().split()\n",
    "\n",
    "def calculate_em(pred: Union[list, str, None], answer: Union[list, str, None], mode: str) -> int:\n",
    "    if mode == 'text' and pred and answer:\n",
    "        pred = normalize_and_tokenize_text(pred)\n",
    "        answer = normalize_and_tokenize_text(answer)\n",
    "        print (pred, answer)\n",
    "        for i in range(0, len(pred) - len(answer) + 1):\n",
    "            print (answer, pred[i: i + len(answer)])\n",
    "            if answer == pred[i: i + len(answer)]:\n",
    "                return 1\n",
    "        return 0\n",
    "    else:\n",
    "        return int(pred == answer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f9d494-df5f-4665-920c-5aa86ec46827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['27'] ['2']\n",
      "['2'] ['27']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_em('27', '2', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf678c-4bfa-48e3-a109-4a1cc81a1586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-vika_kurkin_clone]",
   "language": "python",
   "name": "conda-env-.mlspace-vika_kurkin_clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

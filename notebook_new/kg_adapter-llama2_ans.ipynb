{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f5a1e2-3069-4e4f-b972-ca67e249d6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0e3b902ef64e6c80cead65ac4f93ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a6acfdf0964cdd8ae81e62908ca468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in tokenizer: 32000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from urllib.request import urlopen\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading some sources of the projection adapter and image encoder\n",
    "hf_hub_download(repo_id=\"AIRI-Institute/OmniFusion\", filename=\"models.py\", local_dir='./')\n",
    "from models import CLIPVisionTower\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "PROMPT = \"This is a dialog with AI assistant.\\n\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AIRI-Institute/OmniFusion\", subfolder=\"OmniMistral-tokenizer\", use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"AIRI-Institute/OmniFusion\", subfolder=\"OmniMistral-model\", torch_dtype=torch.bfloat16, device_map=DEVICE)\n",
    "\n",
    "unk_id = tokenizer.encode(\"<unk>\", add_special_tokens=False)[0]\n",
    "tokenizer.pad_token_id = 2\n",
    "tokenizer.eos_token_id = 0\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "N_EMBEDDINGS = model.model.embed_tokens.weight.shape[0]\n",
    "print(\"Number of embeddings in tokenizer:\", N_EMBEDDINGS)\n",
    "\n",
    "projection = torch.load(\"/home/jovyan/shares/SR004.nfs2/chekalina/check_halu/ckpts/projection_llama2_chat\", map_location=DEVICE)\n",
    "start_emb = torch.load(\"/home/jovyan/shares/SR004.nfs2/chekalina/check_halu/ckpts/SOI_llama2_chat.pt\", map_location=DEVICE)\n",
    "end_emb = torch.load(\"/home/jovyan/shares/SR004.nfs2/chekalina/check_halu/ckpts/EOI_llama2_chat.pt\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfbbe95-e58c-4454-9646-43c7cbd52fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.read_csv('/home/jovyan/shares/SR004.nfs2/chekalina/check_halu/83000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c0a5d7-b003-45a5-a536-73ae2f09e46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "tds = Dataset.from_pandas(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c378e1c-ab5a-490e-9f05-0fbf90ea4e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'In what year did the field where the 2016 TaxSlayer Bowl was held assumed its name?',\n",
       " 'answer': '2010',\n",
       " 'ents': \"['2016 TaxSlayer Bowl (January)', 'EverBank Field']\",\n",
       " 'embs': '[[-0.1031, -0.4383, 0.0454, 0.2026, -0.5881, -0.3309, -0.2731, 0.134, 0.0228, -0.0791, 0.0535, -0.4907, 0.2224, -0.126, 0.0423, -0.1931, -0.766, -0.024, 0.2739, 0.4367, -0.5175, 0.3946, -0.2065, 0.4556, -0.2043, 0.0668, -0.1485, -0.3304, 0.3304, 0.1197, -0.0182, 0.1501, 0.1326, -0.0289, 0.123, -0.315, 0.0723, -0.2623, -0.338, 0.3193, -0.2687, -0.0728, -0.5572, 0.1382, 0.084, -0.0601, -0.3014, 0.3619, 0.0376, -0.4504, -0.368, 0.238, -0.3675, 0.1055, -0.8552, 0.1323, 0.4664, -0.2566, 0.017, -0.3725, 0.1266, 0.1788, 0.3019, -0.0084, 0.1141, 0.2377, -0.0046, -0.3341, -0.3334, -0.253, 0.2078, -0.3426, 0.2435, -0.2223, -0.0255, 0.0003, -0.0775, -0.0613, -0.2265, 0.1033, 0.0907, -0.3794, -0.3275, -0.4337, -0.3618, -0.1619, -0.1508, -0.2009, -0.1984, -0.1725, 0.175, 0.3661, -0.0229, -0.1412, 0.0817, -0.2492, -0.0952, -0.3488, -0.1213, -0.2087, 0.2161, -0.2641, -0.1009, 0.2094, 0.0341, -0.1825, 0.1239, 0.3223, -0.1216, 0.1789, -0.2955, -0.3472, -0.046, 0.3313, 0.1121, 0.0794, 0.4396, -0.1116, -0.6431, -0.1474, 0.3141, -0.0418, -0.1978, -0.0652, -0.1876, -0.0276, 0.3085, 0.0924, -0.2749, 0.5645, -0.0614, 0.1209, 0.1923, 0.2003, -0.0641, 0.0902, 0.3454, 0.3993, -0.167, 0.1302, 0.1288, 0.0869, -0.12, 0.2805, -0.3951, -0.1852, 0.1224, -0.1306, 0.2063, -0.0303, -0.0771, -0.2997, 0.1107, -0.3911, -0.0095, 0.2797, -0.2383, 0.282, -0.1847, 0.008, 0.0772, -0.0808, -0.1065, -0.3268, 0.0486, 0.0521, 0.0161, 0.1658, 0.3256, 0.2207, 0.0586, -0.0059, -0.5434, 0.0239, -0.0623, -0.0636, 0.7648, -0.2766, -0.0995, 0.0597, 0.3079, -0.1087, -0.2367, -0.1313, -0.0182, -0.0194, 0.3549, -0.286, 0.3605, 0.4029, 0.0696, -0.1465, 0.1761, 0.5693, 0.1637, -0.5406, 0.2034, -0.3462, 0.0628, 0.2241], -111]'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c97a74b-2caf-4f6f-9b89-de162dfe5f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "lst = literal_eval(tds[10]['embs'])\n",
    "lst = [elem for elem in lst if elem != -111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac3706c-c3b3-42bb-a61d-7f74b9bcf1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ents = self.ds[idx]['ents']\n",
    "        try:\n",
    "            lst = literal_eval(self.ds[idx]['embs'])\n",
    "            lst = [elem for elem in lst if elem != -111]\n",
    "            embs = np.array(lst)\n",
    "            #embs = np.reshape(embs, (-1, 200)) \n",
    "        except:\n",
    "            print (self.ds[idx]['embs'])\n",
    "        \n",
    "        return self.ds[idx]['question'], self.ds[idx]['answer'], ents, embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ba91ac-9ff5-4e45-a2a9-042c5a8dde08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = PretrainDataset(tds)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e06bac0-8ec3-443f-944f-cf7b2ca84768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a, b, c, d = dataset[1777]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd71601b-54b7-48c1-873d-7497ebbafbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a021d918-3f2d-4e27-82ea-93db66662ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(d).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaad73a-8d76-43ab-9b13-b5dd647e5243",
   "metadata": {},
   "source": [
    "### Train adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe11e5b-d8d0-44b4-bfbd-70594592ecc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_words_ids = tokenizer([\"\\n\", \"</s>\", \":\"], add_special_tokens=False).input_ids + [[13]]\n",
    "gen_params = {\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"early_stopping\": False,\n",
    "        \"num_beams\": 3,\n",
    "        \"repetition_penalty\": 2.0,\n",
    "        \"remove_invalid_values\": True,\n",
    "        \"eos_token_id\": 0,\n",
    "        \"pad_token_id\": 2,\n",
    "        \"forced_eos_token_id\": 0,\n",
    "        \"use_cache\": True,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"bad_words_ids\": bad_words_ids,\n",
    "        \"num_return_sequences\": 3,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c3bdf57-ac02-4d09-b16d-54f2b008e4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader) // 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b551fbeb-6ebb-4081-81bf-7a523c44398d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m mstral_emb_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#start_emb = torch.normal(torch.zeros(mstral_emb_dim), torch.ones(mstral_emb_dim) / mstral_emb_dim**0.5).to(device=DEVICE, dtype=model.dtype)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#end_emb = torch.normal(torch.zeros(mstral_emb_dim), torch.ones(mstral_emb_dim) / mstral_emb_dim**0.5).to(device=DEVICE, dtype=model.dtype)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#projection = nn.Linear(kg_emb_dim, mstral_emb_dim).to(device=DEVICE, dtype=model.dtype)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mstart_emb\u001b[49m\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[1;32m     19\u001b[0m end_emb\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[1;32m     20\u001b[0m projection\u001b[38;5;241m.\u001b[39mrequires_grad_()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_emb' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import (Adafactor, AdafactorSchedule,\n",
    "                                       get_cosine_schedule_with_warmup)\n",
    "\n",
    "import gc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "kg_emb_dim = 200\n",
    "mstral_emb_dim = 4096\n",
    "\n",
    "#start_emb = torch.normal(torch.zeros(mstral_emb_dim), torch.ones(mstral_emb_dim) / mstral_emb_dim**0.5).to(device=DEVICE, dtype=model.dtype)\n",
    "#end_emb = torch.normal(torch.zeros(mstral_emb_dim), torch.ones(mstral_emb_dim) / mstral_emb_dim**0.5).to(device=DEVICE, dtype=model.dtype)\n",
    "#projection = nn.Linear(kg_emb_dim, mstral_emb_dim).to(device=DEVICE, dtype=model.dtype)\n",
    "\n",
    "start_emb.requires_grad_()\n",
    "end_emb.requires_grad_()\n",
    "projection.requires_grad_()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "    \n",
    "lr = 5e-3\n",
    "weight_decay = 1e-5\n",
    "trainable_parameters = [start_emb] + [end_emb] + list(projection.parameters())\n",
    "\n",
    "opt = AdamW(trainable_parameters, lr=lr, weight_decay=weight_decay)\n",
    "loss_fct = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=unk_id)\n",
    "\n",
    "grad_accum = 256\n",
    "\n",
    "loss_best = 1000.0\n",
    "\n",
    "losses = []\n",
    "losses_batch = []\n",
    "iters = 0\n",
    "n_iters = len(dataloader)\n",
    "scheduler = get_cosine_schedule_with_warmup(opt, num_warmup_steps=10, num_training_steps=n_iters // grad_accum)\n",
    "\n",
    "for epoch in range(1):\n",
    "    i = 0 \n",
    "    for step in tqdm.notebook.tqdm(range(n_iters)):\n",
    "        batch = next(iter(dataloader))\n",
    "        question, answer, ents, embs = batch\n",
    "        model.eval()\n",
    "        model.requires_grad = False\n",
    "        #opt.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            #print (question, answer)\n",
    "            text_ids_in = tokenizer.encode(question[0], add_special_tokens=False, return_tensors=\"pt\").to(device=DEVICE)\n",
    "            text_ids_out = tokenizer.encode(answer[0], add_special_tokens=False, return_tensors=\"pt\").to(device=DEVICE)\n",
    "            input_embeddings = model.model.embed_tokens(text_ids_in)\n",
    "            output_embeddings = model.model.embed_tokens(text_ids_out)\n",
    "            \n",
    "            #output_embeddings = model.model.embed_tokens(text_ids[...,:text_ids.shape[1]//2])\n",
    "            #output_embeddings = model.model.embed_tokens(text_ids[...,text_ids.shape[1]//2+1:])\n",
    "            \n",
    "        try:\n",
    "            m = embs.mean(2, keepdim=True)\n",
    "            s = embs.std(2, unbiased=False, keepdim=True)\n",
    "            embs -= m\n",
    "            embs /= s\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        try:\n",
    "            projected_kg_embeddings = projection(embs.to(\n",
    "                        device=DEVICE, dtype=model.dtype\n",
    "                    ))\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            print (\"embs.shape\", embs.shape)\n",
    "            continue\n",
    "        \n",
    "        embeddings1 = torch.cat(\n",
    "                [\n",
    "                    input_embeddings,\n",
    "                    start_emb[None, None, ...],\n",
    "                    projected_kg_embeddings,\n",
    "                    end_emb[None, None, ...]\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        \n",
    "        gen_params['max_new_tokens'] = embeddings1.shape[1]\n",
    "        \n",
    "        #mask = torch.full(embeddings1.shape, False)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=model.dtype):\n",
    "            logits = model(inputs_embeds=torch.cat(\n",
    "                [\n",
    "                    input_embeddings,\n",
    "                    start_emb[None, None, ...],\n",
    "                    projected_kg_embeddings,\n",
    "                    end_emb[None, None, ...],\n",
    "                    output_embeddings\n",
    "                ],\n",
    "                dim=1,\n",
    "            ), output_hidden_states=True).get(\"logits\")\n",
    "            \n",
    "            num_output_tokens = output_embeddings.shape[1]\n",
    "            logits = logits[..., -num_output_tokens:, :].contiguous()\n",
    "            labels = text_ids_out.contiguous()\n",
    "            loss = loss_fct(logits.permute(0, 2, 1), labels).mean()\n",
    "            print (\"logits.shape\", logits.shape)\n",
    "            print (\"labels.shape\", labels.shape)\n",
    "            #shift_logits = logits[..., :-1, :].contiguous()\n",
    "            #shift_labels = labels[..., 1:].contiguous()\n",
    "            #print (\"logits.shape\", shift_logits.shape)\n",
    "            #print (\"shift_labels\", shift_labels.shape)\n",
    "            #print (\"output_embeddings\", output_embeddings.shape)\n",
    "            \n",
    "            #labels = labels[...,:text_ids.shape[1]//2]\n",
    "            \n",
    "            #mask = mask[:, -output_embeddings.shape[1]:]\n",
    "        \n",
    "            #print (\"logits.shape\", logits.shape)\n",
    "            #print (\"labels.shape\", labels.shape)\n",
    "            #loss = loss_fct(shift_logits.permute(0, 2, 1), shift_labels).mean()\n",
    "            #print (loss)\n",
    "            \n",
    "        if model.dtype == torch.float16:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        losses_batch.append(loss.item())\n",
    "        \n",
    "        if (step % (2*grad_accum) == 0):  \n",
    "            print (step)\n",
    "            out = model.generate(inputs_embeds=embeddings1, **gen_params)\n",
    "            #print (\"out.shape\", out.shape)\n",
    "            #print (\"projected_kg_embeddings shape\", projected_kg_embeddings.shape)\n",
    "            #out = out[:, 1:]\n",
    "            #print (\"out.shape\", out.shape)\n",
    "            generated_texts = tokenizer.batch_decode(out)[0]\n",
    "            print (question[0])\n",
    "            print (\"\\n last part \\n\")\n",
    "            print (answer[0])\n",
    "            print (\"\\n continue\", generated_texts)\n",
    "            print (\"\\n\")\n",
    "            \n",
    "            print (\"loss\", np.mean(losses_batch))\n",
    "            print ('lr', scheduler.get_lr()[0], step, flush = True)\n",
    "            plt.title(\"train loss\\n\" + f\"\\n\\nEpoch [{epoch}], iter [{iters}/{n_iters}]\")\n",
    "            accum_loss = np.mean(losses_batch)\n",
    "            losses.append(accum_loss)\n",
    "            plt.semilogy(losses)\n",
    "            plt.grid()\n",
    "            plt.savefig(f\"ckpts/loss3.png\")\n",
    "            plt.close(\"all\")\n",
    "\n",
    "\n",
    "        if iters % grad_accum == 0 and iters > 0:\n",
    "            if model.dtype == torch.float16:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                opt.step()\n",
    "            opt.zero_grad()\n",
    "            scheduler.step()\n",
    "            accum_loss = np.mean(losses_batch)\n",
    "            losses.append(accum_loss)\n",
    "            losses_batch = []\n",
    "\n",
    "            if accum_loss < loss_best:\n",
    "                loss_best = accum_loss\n",
    "                torch.save(projection, f\"ckpts/projection_llama2_chat_qa1\")\n",
    "                torch.save(start_emb, f\"ckpts/SOI2_llama2_chat_qa1.pt\")\n",
    "                torch.save(end_emb, f\"ckpts/EOI2_llama2_chat_qa1.pt\")\n",
    "            \n",
    "            \n",
    "            #gc.collect()\n",
    "        \n",
    "        iters += 1\n",
    "\n",
    "        # model inference to get\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0a9505b-f154-45f9-a3c3-3d23ae6eab63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9207,  3225, 28708,   304, 10852,   385,  1124, 11961,  2895, 28725,\n",
       "           318, 28723, 28741, 28723,   340,   334, 28723, 28790, 28723,   325,\n",
       "          4637,   385, 28731,   349,   264, 17092,     0],\n",
       "        [ 9207,  3225, 28708,   304, 10852,   385,  1124, 11961,  2895, 28725,\n",
       "           318, 28723, 28741, 28723,   340,   334, 28723, 28790, 28723,   325,\n",
       "          4637,   385,  5046, 28731,   460,   272,     0],\n",
       "        [ 9207,  3225, 28708,   304, 10852,   385,  1124, 11961,  2895, 28725,\n",
       "           318, 28723, 28741, 28723,   340,   334, 28723, 28790, 28723,   325,\n",
       "          4637,   385,  5046, 28731,   297, 13895,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(inputs_embeds=embeddings1, **gen_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "011c5d7d-0bc6-4f32-b92b-94d742b34925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings1 = torch.cat(\n",
    "                [\n",
    "                    input_embeddings,\n",
    "                    start_emb[None, None, ...],\n",
    "                    projected_kg_embeddings,\n",
    "                    end_emb[None, None, ...],\n",
    "                    output_embeddings\n",
    "                ],\n",
    "                dim=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d33f8-0baf-417e-bfba-bd7d36a7ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(inputs_embeds=embeddings1,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=150,\n",
    "    top_p=0.82,\n",
    "    top_k=0,\n",
    "    eos_token_id= 0,\n",
    "    pad_token_id=2,\n",
    "    temperature=3.5)\n",
    "\n",
    "print (\"out.shape\", out.shape)\n",
    "out = out[:, 1:]\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(out)[0]\n",
    "\n",
    "generated_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f3ed3-2cdb-4bc1-9733-f0afd777a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params = {\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": 150,\n",
    "        \"early_stopping\": False,\n",
    "        \"num_beams\": 3,\n",
    "        \"repetition_penalty\": 2.0,\n",
    "        \"remove_invalid_values\": True,\n",
    "        \"eos_token_id\": 0,\n",
    "        \"pad_token_id\": 2,\n",
    "        \"forced_eos_token_id\": 0,\n",
    "        \"use_cache\": True,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"bad_words_ids\": bad_words_ids,\n",
    "        \"num_return_sequences\": 3,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de77c14-202b-4e5f-bbaf-79ae9aac4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(inputs_embeds=embeddings1,\n",
    "    **gen_params)\n",
    "\n",
    "print (\"out.shape\", out.shape)\n",
    "#out = out[:, 1:]\n",
    "\n",
    "generated_texts = tokenizer.decode(out)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db322041-00ef-47f4-ae0c-0652053d6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f892941-5050-49c9-b0bc-2f158d170fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-vika_kurkin_clone]",
   "language": "python",
   "name": "conda-env-.mlspace-vika_kurkin_clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
